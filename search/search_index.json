{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to TabbyAPI","text":"<p>Important</p> <p>This documentation is under construction. URLs may change at any time. Thanks!</p> <p>Welcome to TabbyAPI!</p> <p>This wiki aims to provide a place for documenting various aspects of this project.</p> <p>Not sure where to start? Check out the Getting Started page.</p> <p>Are you a developer? Take a look at the API page and autogenerated documentation</p> <p>Have issues? Check out the FAQ page.</p>"},{"location":"10.-Tool-Calling/","title":"Tool Calling in TabbyAPI","text":"<p>Note</p> <p>Before getting started here, please look at the Custom templates page for foundational concepts.</p> <p>Thanks to Storm for creating this documentation page.</p> <p>TabbyAPI's tool calling implementation aligns with the OpenAI Standard, following the OpenAI Tools Implementation closely.</p>"},{"location":"10.-Tool-Calling/#features-and-limitations","title":"Features and Limitations","text":"<p>TabbyAPI's tool implementation supports: - Tool calling when streaming - Calling multiple tools per turn</p> <p>Current limitations: - No support for <code>tool_choice</code> parameter (always assumed to be auto) - <code>strict</code> parameter not yet supported (OAI format ensured, but dtype and argument name choices not yet enforced)</p>"},{"location":"10.-Tool-Calling/#model-support","title":"Model Support","text":"<p>TabbyAPI exposes controls within the <code>prompt_template</code> to accommodate models specifically tuned for tool calling and those that aren't. By default, TabbyAPI includes <code>chatml_with_headers_tool_calling.jinja</code>, a generic template built to support the Llama 3.1 family and other models following the ChatML (with headers) format.</p> <p>For more templates, check out llm-prompt-templates.</p>"},{"location":"10.-Tool-Calling/#usage","title":"Usage","text":"<p>In order to use tool calling in TabbyAPI, you must select a <code>prompt_template</code> that supports tool calling when loading your model. </p> <p>For example, if you are using a Llama 3.1 Family model you can simply modify your <code>config.yml</code>'s <code>prompt_template:</code> to use the default tool calling template like so:</p> <pre><code>model:\n   ...\n   prompt_template: chatml_with_headers_tool_calling\n</code></pre> <p>If loading via <code>/v1/model/load</code>, you would also need to specify a tool-supporting <code>prompt_template</code>.</p>"},{"location":"10.-Tool-Calling/#creating-a-tool-calling-prompt-template","title":"Creating a Tool Calling Prompt Template","text":"<p>Here's how to create a TabbyAPI tool calling prompt template:</p> <ol> <li> <p>Define proper metadata:</p> <p>Tool Call supporting <code>prompt_templates</code> can have the following fields as metadata: - <code>tool_start</code> This is a string that we expect the model to write when initating a tool call. (Required) - <code>tool_end</code> This is a string the model expects after completing a tool call.</p> <p>Here is an example of these being defined:</p> </li> </ol> <pre><code>{# Metadata #} \n{% set stop_strings = [\"&lt;|im_start|&gt;\", \"&lt;|im_end|&gt;\"] %}\n{% set message_roles = ['system', 'user', 'assistant', 'tool'] %}\n{% set tool_start = \"&lt;|tool_start|&gt;\" %}\n{% set tool_end = \"&lt;|tool_end|&gt;\" %}\n</code></pre> <p><code>tool_start</code> and <code>tool_end</code> should be selected based on which model you decide to use. For example, Groq's Tool calling models expects <code>&lt;tool_call&gt;</code> and <code>&lt;/tool_call&gt;</code> while Llama3 FireFunctionV2's model expects only <code>functools</code> to start the call, without a <code>tool_end</code></p> <ol> <li>Define an <code>initial_system_prompt</code>:</li> </ol> <p>While the name of your <code>inital_system_prompt</code> can vary, it's purpose does not. This inital prompt is typically a simple instruction set followed by accessing the <code>tools_json</code> variable. This will contain the function specification the user provided to the <code>tools</code> endpoint in their client when the chat completion request. Inside the template we can call this like so: <code>{{ tools_json }}</code>.</p> <p>Note: Depending on the model you are using, it's possible your model may expect a special set of tokens to surround the function specifications. Feel free to surround <code>tools_json</code> with these tokens.</p> <pre><code>{% set initial_system_prompt %}\nYour instructions here...\nAvailable functions:\n{{ tools_json }}\n{% endset %}\n</code></pre> <p>You'll then want to make sure to provide this to the model in the first message it recieves. Here is a simple example:</p> <pre><code>{%- if loop.first -%}\n{{ bos_token }}{{ start_header }}{{ role }}{{ end_header }}\n{{ inital_system_prompt }}\n\n{{ content }}{{ eos_token }}\n</code></pre> <ol> <li>Handle messages with the <code>tool</code> role:</li> </ol> <p>After a tool call is made, a well behaved client will respond to the model with a new message containing the role <code>tool</code>. This is a response to a tool call containing the results of it's execution.</p> <p>The simplest implementation of this will be to ensure your <code>message_roles</code> list within your prompt template contains <code>tool</code>. Further customization may be required for models that expect specific tokens surrounding tool reponses. An example of this customization is the Groq family of models from above. They expect special tokens surrounding their tool responses such as:</p> <pre><code>{% if role == 'tool' %}\n&lt;tool_response&gt;{{ content }}&lt;/tool_response&gt;\n{% endif %}\n</code></pre> <ol> <li>Preserve tool calls from prior messages:</li> </ol> <p>When creating a tool calling <code>prompt_template</code>, ensure you handle previous tool calls from the model gracefully. Each <code>message</code> object within <code>messages</code> exposed within the <code>prompt_template</code> could also contain <code>tool_calls_json</code>. This field will contain tool calls made by the assistant in previous turns, and must be handled appropriatly so that the model understands what previous actions it has taken (and can properly identify what tool response ID belongs to which call).</p> <p>This will require using the <code>tool_start</code> (and possibly <code>tool_end</code>) from above to wrap the <code>tool_call_json</code> like so:</p> <pre><code>{% if 'tool_calls_json' in message and message['tool_calls_json'] %}\n{{ tool_start }}{{ message['tool_calls_json'] }}{{ tool_end }}\n{% endif %}\n</code></pre> <ol> <li>Handle tool call generation:</li> </ol> <pre><code>{% set tool_reminder %}\nAvailable Tools:\n{{ tools_json }}\n\nTool Call Format Example:\n{{ tool_start }}{{ example_tool_call }}\n\nPrefix &amp; Suffix: Begin tool calls with {{ tool_start }} and end with {{ tool_end }}.\nArgument Types: Use correct data types for arguments (e.g., strings in quotes, numbers without).\n{% endset %}\n\n{% if tool_precursor %}\n{{ start_header }}system{{ end_header }}\n{{ tool_reminder }}{{ eos_token }}\n{{ start_header }}assistant{{ end_header }}\n{{ tool_precursor }}{{ tool_start }}\n{% else %}\n{{ start_header }}assistant{{ end_header }}\n{% endif %}\n</code></pre> <p>This clever bit of temporal manipulation allows us to slip in a reminder as a system message right before the model generates a tool call, but after it writes the <code>tool_start</code> token. This is possible due to TabbyAPI revisitng the <code>prompt_template</code> after a <code>tool_start</code> token is detected. Here's how it works:</p> <ul> <li>We detect <code>tool_precursor</code>, which signals the model is about to generate a tool call.</li> <li>We then inject a system message with our <code>tool_reminder</code>.</li> <li>Finally, we initialize an assistant message using <code>tool_precursor</code> as the content.</li> </ul> <p>This creates the illusion that the model just happened to remember the available tools and proper formatting right before generating the tool call. It's like giving the model a little nudge at exactly the right moment, enhancing its performance without altering what the user sees.</p> <p>When creating your own tool calling <code>prompt_template</code>, it's best to reference the default <code>chatml_with_headers_tool_calling.jinja</code> template as a starting point.</p>"},{"location":"10.-Tool-Calling/#support-and-bug-reporting","title":"Support and Bug Reporting","text":"<p>For bugs, please create a detailed issue with the model, prompt template, and conversation that caused it. Alternatively, join our Discord and ask for Storm.</p>"},{"location":"AI-Horde/","title":"Kobold AI Horde","text":"<p>The AI Horde is a FOSS crowdsourced compute pool to serve models to users who are not able to access them. TabbyAPI is currently the only horde compatible LLM server that runs on Windows with parallel batching.</p> <p>To get started, Horde requires an API key, which you can acquire here</p> <p>The LLM branch of AI Horde does not use the OpenAI standard, but uses KoboldAI's API. Here are the steps to configure your TabbyAPI instance for hosting:</p> <ol> <li>In <code>config.yml</code>, set the <code>api_servers</code> value to include <code>\"Kobold\"</code> which will enable the KoboldAI API.</li> <li>Horde doesn't support API key authentication. Therefore, you need to enable <code>disable_auth</code> in <code>config.yml</code></li> <li>After those config changes, launch TabbyAPI as normal and your server should be Horde-ready.</li> </ol> <p>Now, the horde needs a \"worker\" to interface between its servers and your TabbyAPI backend. To accomplish that, here are the steps:</p> <ol> <li>Clone the AI horde worker repository</li> <li><code>git clone https://github.com/Haidra-Org/AI-Horde-Worker</code></li> <li>Open the repository in your favorite editor and copy <code>bridgeData_template.yaml</code> to <code>bridgeData.yaml</code>. This will serve as your configuration file.</li> </ol> <p>Afterwards, you'd need to adjust the options based on your preferences. Here are the recommended values to adjust:</p> <ul> <li><code>api_key</code>: The Horde API key you registered for above.</li> <li><code>max_threads</code>: How many requests should be run at once? A higher value should also use a higher batch size and cache size in Tabby.</li> <li><code>scribe_name</code>: The name of your worker that's displayed in Horde.</li> <li><code>kai_url</code>: The URL to your TabbyAPI backend. This will usually be <code>http://localhost:5000</code></li> <li><code>max_length</code>: The maximum number of tokens for every request. 512 is recommended.</li> <li><code>max_context_length</code>: The maximum context length of the worker itself. It's recommended to set this to the model's <code>max_seq_len</code>.</li> </ul> <p>Finally, launch the worker by running <code>.\\horde-scribe-bridge.bat</code> or <code>./horde-scribe-bridge.sh</code> depending on your system.</p>"},{"location":"Chat-Completions/","title":"Templates","text":""},{"location":"Chat-Completions/#chat-completions","title":"Chat Completions","text":"<p>TabbyAPI builds on top of the HuggingFace \"chat templates\" standard for OAI style chat completions (<code>/v1/chat/completions</code>).</p> <p>If you'd like more detail, look at the autogenerated documentation.</p>"},{"location":"Chat-Completions/#custom-templates","title":"Custom Templates","text":"<p>By default, TabbyAPI will try to pull the chat template from a model's <code>chat_template</code> key within  a model's <code>tokenizer_config.json</code>, but you can also make a custom jinja file. To learn how to create a HuggingFace compatible jinja2 template, Please read Huggingface's documentation.</p> <p>If you create a custom template for a model, consider PRing it to the templates repository</p> <p>In addition, there's also support to specify stopping strings within the chat template. This can be achieved by adding <code>{%- set stop_strings = [\"string1\"] -%}</code> at the top of the jinja file. In this case, <code>string1</code> will be appended to your completion as a stopping string.</p> <p>Warning</p> <p>Make sure to add <code>{%- -%}</code> for any top-level metadata. If this is not provided, the top of the rendered prompt will have extra whitespace. This does not apply for comments <code>{# #}</code></p> <p>To use a custom template, place it in the templates folder, and make sure to set the <code>prompt_template</code> field in <code>config.yml</code> (see model config) to the template's filename.</p>"},{"location":"Chat-Completions/#template-variables","title":"Template Variables","text":"<p>A chat completions request to TabbyAPI also supports custom template variables in the form of a key/value object in the JSON body. Here's an example:</p> <pre><code>\"template_vars\": {\n    \"test_var\": \"hello!\"\n}\n</code></pre> <p>Now let's pass the custom var in the following template:</p> <pre><code>I'm going to say {{ test_var }}\n</code></pre> <p>Running render on this template will now result in: <code>I'm going to say hello!</code></p>"},{"location":"Community-Projects/","title":"Community Projects","text":"<p>Due to TabbyAPI solely being an API server, the community has provided custom-tailored applications to communicate with a TabbyAPI backend.</p> <p>Note</p> <p>If you would like to showcase a community project, feel free to contact a dev!</p> <p>Below is a list of projects that integrate TabbyAPI:</p> <ul> <li>TabbyAPI Gradio Loader by Doctor Shotgun - Gradio WebUI for loading/unloading models and loras via TabbyAPI.</li> <li>ST TabbyAPI Loader by kingbri - SillyTavern extension for loading and unloading models via TabbyAPI.</li> <li>SillyTavern Launcher by deffcolony - CLI manager for AI tools which includes packaged setup and management of TabbyAPI.</li> <li>SillyTavern by Cohee and RossAscends - An AI frontend for power users that fully supports TabbyAPI's generation options.</li> <li>Agnaistic by Sceik - Multi-user AI role-play chat frontend that fully supports TabbyAPI's generation options.</li> <li>DS-LLM-WebUI by Doctor Shotgun - A tool use assistant for local LLMs which is fully designed around TabbyAPI.</li> <li>PolyMind by Itsme2417 - A multimodal function calling LLM webui that is designed to be used with TabbyAPI.</li> </ul>"},{"location":"FAQ/","title":"FAQ","text":"<ul> <li>What OS is supported?</li> <li> <p>Windows and Linux</p> </li> <li> <p>I'm confused, how do I do anything with this API?</p> </li> <li> <p>That's okay! Not everyone is an AI mastermind on their first try. The start scripts and <code>config.yml</code> aim to guide new users to the right configuration. The Usage page explains how the API works. Community Projects contain UIs that help interact with TabbyAPI via API endpoints. The Discord Server is also a place to ask questions, but please be nice.</p> </li> <li> <p>How do I interface with the API?</p> </li> <li> <p>The wiki is meant for user-facing documentation. Devs are recommended to use the autogenerated documentation for OpenAI and Kobold servers</p> </li> <li> <p>What does TabbyAPI run?</p> </li> <li> <p>TabbyAPI uses Exllamav2 as a powerful and fast backend for model inference, loading, etc. Therefore, the following types of models are supported:</p> <ul> <li>Exl2 (Highly recommended)</li> <li>GPTQ</li> <li>FP16 (using Exllamav2's loader)</li> </ul> </li> <li> <p>Exllamav2 may error with the following exception: <code>ImportError: DLL load failed while importing exllamav2_ext: The specified module could not be found.</code></p> </li> <li>First, make sure to check if the wheel is equivalent to your python version and CUDA version. Also make sure you're in a venv or conda environment.</li> <li>If those prerequisites are correct, the torch cache may need to be cleared. This is due to a mismatching exllamav2_ext.<ul> <li>In Windows: Find the cache at <code>C:\\Users\\&lt;User&gt;\\AppData\\Local\\torch_extensions\\torch_extensions\\Cache</code> where <code>&lt;User&gt;</code> is your Windows username</li> <li>In Linux: Find the cache at <code>~/.cache/torch_extensions</code></li> <li>look for any folder named <code>exllamav2_ext</code> in the python subdirectories and delete them.</li> <li>Restart TabbyAPI and launching should work again.</li> </ul> </li> </ul>"},{"location":"Getting-Started/","title":"Getting Started","text":""},{"location":"Getting-Started/#prerequisites","title":"Prerequisites","text":"<p>To get started, make sure you have the following installed on your system:</p> <ul> <li> <p>Python 3.x (preferably 3.11) with pip</p> <ul> <li>Do NOT install python from the Microsoft store! This will cause issues with pip.</li> <li>Alternatively, you can use miniconda if it's present on your system.</li> </ul> </li> </ul> <p>Note</p> <p>You can install miniconda3 on your system which will give you the benefit of having both python and conda!</p> <p>Warning</p> <p>CUDA and ROCm aren't prerequisites because torch can install them for you. However, if this doesn't work (ex. DLL load failed), install the CUDA toolkit or ROCm on your system.</p> <ul> <li>CUDA 12.x</li> <li>CUDA 11.8</li> <li>ROCm 6.0</li> </ul> <p>Warning</p> <p>Sometimes there may be an error with Windows that VS build tools needs to be installed. This means that there's a package that isn't supported for your python version. You can install VS build tools 17.8 and build the wheel locally. In addition, open an issue stating that a dependency is building a wheel.</p>"},{"location":"Getting-Started/#installing","title":"Installing","text":""},{"location":"Getting-Started/#for-beginners","title":"For Beginners","text":"<ol> <li> <p>Clone this repository to your machine: <code>git clone https://github.com/theroyallab/tabbyAPI</code></p> </li> <li> <p>Navigate to the project directory: <code>cd tabbyAPI</code></p> </li> <li> <p>Run the appropriate start script (<code>start.bat</code> for Windows and <code>start.sh</code> for linux).</p> <ol> <li>Follow the on-screen instructions and select the correct GPU library.</li> <li>Assuming that the prerequisites are installed and can be located, a virtual environment will be created for you and dependencies will be installed.</li> </ol> </li> <li> <p>The API should start with no model loaded</p> </li> </ol>"},{"location":"Getting-Started/#for-advanced-users","title":"For Advanced Users","text":"<p>Note</p> <p>TabbyAPI has recently switched to use pyproject.toml. These instructions may look different than before.</p> <ol> <li>Follow steps 1-2 in the For Beginners section</li> <li>Create a python environment through venv:<ol> <li><code>python -m venv venv</code></li> <li>Activate the venv<ol> <li>On Windows: <code>.\\venv\\Scripts\\activate</code></li> <li>On Linux: <code>source venv/bin/activate</code></li> </ol> </li> </ol> </li> <li>Install the pyproject features based on your system:<ol> <li>Cuda 12.x: <code>pip install -U .[cu121]</code></li> <li>Cuda 11.8: <code>pip install -U .[cu118]</code></li> <li>ROCm 5.6: <code>pip install -U .[amd]</code></li> </ol> </li> <li>Start the API by either<ol> <li>Run <code>start.bat/sh</code>. The script will check if you're in a conda environment and skip venv checks.</li> <li>Run <code>python main.py</code> to start the API. This won't automatically upgrade your dependencies.</li> </ol> </li> </ol>"},{"location":"Getting-Started/#configuration","title":"Configuration","text":"<p>Loading solely the API may not be your optimal usecase. Therefore, a config.yml exists to tune initial launch parameters and other configuration options.</p> <p>A config.yml file is required for overriding project defaults. If you are okay with the defaults, you don't need a config file!</p> <p>If you do want a config file, copy over <code>config_sample.yml</code> to <code>config.yml</code>. All the fields are commented, so make sure to read the descriptions and comment out or remove fields that you don't need.</p> <p>In addition, if you want to manually set the API keys, copy over <code>api_keys_sample.yml</code> to <code>api_keys.yml</code> and fill in the fields. However, doing this is less secure and autogenerated keys should be used instead.</p> <p>You can also access the configuration parameters under 2. Configuration in this wiki!</p>"},{"location":"Getting-Started/#where-next","title":"Where next?","text":"<ol> <li>Take a look at the usage docs</li> <li>Get started with community projects: Find loaders, UIs, and more created by the wider AI community. Any OAI compatible client is also supported.</li> </ol>"},{"location":"Getting-Started/#updating","title":"Updating","text":"<p>There are a couple ways to update TabbyAPI:</p> <ol> <li>Update scripts - Inside the update_scripts folder, you can run the following scripts:<ol> <li><code>update_deps</code>: Updates dependencies to their latest versions.</li> <li><code>update_deps_and_pull</code>: Updates dependencies and pulls the latest commit of the Github repository.</li> </ol> </li> </ol> <p>These scripts exit after running their respective tasks. To start TabbyAPI, run <code>start.bat</code> or <code>start.sh</code>.</p> <ol> <li>Manual - Install the pyproject features and update dependencies depending on your GPU:<ol> <li><code>pip install -U .[cu121]</code> = CUDA 12.x</li> <li><code>pip install -U .[cu118]</code> = CUDA 11.8</li> <li><code>pip install -U .[amd]</code> = ROCm 6.0</li> </ol> </li> </ol> <p>If you don't want to update dependencies that come from wheels (torch, exllamav2, and flash attention 2), use <code>pip install .</code> or pass the <code>--nowheel</code> flag when invoking the start scripts.</p>"},{"location":"Getting-Started/#update-exllamav2","title":"Update Exllamav2","text":"<p>Warning</p> <p>These instructions are meant for advanced users.</p> <p>Important</p> <p>If you're installing a custom Exllamav2 wheel, make sure to use <code>pip install .</code> when updating! Otherwise, each update will overwrite your custom exllamav2 version.</p> <p>NOTE:</p> <ul> <li>TabbyAPI enforces the latest Exllamav2 version for compatibility purposes.</li> <li>Any upgrades using a pyproject gpu lib feature will result in overwriting your installed wheel.<ul> <li>To fix this, change the feature in <code>pyproject.toml</code> locally, create an issue or PR, or install your version of exllamav2 after upgrades.</li> </ul> </li> </ul> <p>Here are ways to install exllamav2:</p> <ol> <li>From a wheel/release (Recommended)<ol> <li>Find the version that corresponds with your cuda and python version. For example, a wheel with <code>cu121</code> and <code>cp311</code> corresponds to CUDA 12.1 and python 3.11</li> </ol> </li> <li>From pip: <code>pip install exllamav2</code><ol> <li>This is a JIT compiled extension, which means that the initial launch of tabbyAPI will take some time. The build may also not work due to improper environment configuration.</li> </ol> </li> <li>From source</li> </ol>"},{"location":"Getting-Started/#other-installation-methods","title":"Other installation methods","text":"<p>These are short-form instructions for other methods that users can use to install TabbyAPI.</p> <p>Warning</p> <p>Using methods other than venv may not play nice with startup scripts. Using these methods indicates that you're an advanced user and know what you're doing.</p>"},{"location":"Getting-Started/#conda","title":"Conda","text":"<ol> <li>Install Miniconda3 with python 3.11 as your base python</li> <li>Create a new conda environment <code>conda create -n tabbyAPI python=3.11</code></li> <li>Activate the conda environment <code>conda activate tabbyAPI</code></li> <li>Install optional dependencies if they aren't present<ol> <li>CUDA via<ol> <li>CUDA 12 - <code>conda install -c \"nvidia/label/cuda-12.2.2\" cuda</code></li> <li>CUDA 11.8 - <code>conda install -c \"nvidia/label/cuda-11.8.0\" cuda</code></li> </ol> </li> <li>Git via <code>conda install -k git</code></li> </ol> </li> <li>Clone TabbyAPI via <code>git clone https://github.com/theroyallab/tabbyAPI</code></li> <li>Continue installation steps from:<ol> <li>For Beginners - Step 3. The start scripts detect if you're in a conda environment and skips the venv check.</li> <li>For Advanced Users - Step 3</li> </ol> </li> </ol>"},{"location":"Getting-Started/#docker","title":"Docker","text":"<ol> <li>Install Docker and docker compose from the docs</li> <li>Install the Nvidia container compatibility layer<ol> <li>For Linux: Nvidia container toolkit</li> <li>For Windows: Cuda Toolkit on WSL</li> </ol> </li> <li>Clone TabbyAPI via <code>git clone https://github.com/theroyallab/tabbyAPI</code></li> <li>Enter the tabbyAPI directory by <code>cd tabbyAPI</code>.<ol> <li>Optional: Set up a config.yml or api_tokens.yml (configuration)</li> </ol> </li> <li>Update the volume mount section in the <code>docker/docker-compose.yml</code> file</li> </ol> <pre><code>volumes:\n\u00a0 # - /path/to/models:/app/models \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Change me\n\u00a0 # - /path/to/config.yml:/app/config.yml \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Change me\n\u00a0 # - /path/to/api_tokens.yml:/app/api_tokens.yml \u00a0 \u00a0 \u00a0 # Change me\n</code></pre> <ol> <li>Optional: If you'd like to build the dockerfile from source, follow the instructions below in <code>docker/docker-compose.yml</code>:</li> </ol> <pre><code>\u00a0 \u00a0 # Uncomment this to build a docker image from source\n\u00a0 \u00a0 #build:\n\u00a0 \u00a0 # \u00a0context: ..\n\u00a0 \u00a0 # \u00a0dockerfile: ./docker/Dockerfile\n\n\u00a0 \u00a0 # Comment this to build a docker image from source\n\u00a0 \u00a0 image: ghcr.io/theroyallab/tabbyapi:latest\n</code></pre> <ol> <li>Run <code>docker compose -f docker/docker-compose.yml up</code> to build the dockerfile and start the server.</li> </ol>"},{"location":"Sampling/","title":"Samplers","text":"<p>Samplers are used to alter raw probabilities during response generations. Users can tune these to adjust what outputs they get.</p> <p>Note</p> <p>Sampling is not a catch-all solution if your generations are behaving the wrong way! These factors can also fall to the prompt, frontend, model, etc. Please do not set arbitrary sampler values without understanding what they do first!</p>"},{"location":"Sampling/#penalties","title":"Penalties","text":"<p>Repetition Penalty -</p> <ul> <li> <p>API request field: <code>repetition_penalty</code></p> </li> <li> <p>Default: <code>1.0</code> - Off</p> </li> <li> <p>Description: Multiplicative method of preventing repetition of previous tokens in the context.</p> </li> </ul> <p>Frequency Penalty -</p> <ul> <li> <p>API request field: <code>frequency_penalty</code></p> </li> <li> <p>Default: <code>0.0</code> - Off</p> </li> <li> <p>Description: A constant value added each time each time a token is sampled, reducing the probability for that specific token.</p> </li> </ul> <p>Presence Penalty -</p> <ul> <li> <p>API request field: <code>presence_penalty</code></p> </li> <li> <p>Default: <code>0.0</code> - Off</p> </li> <li> <p>Description: Additive method of preventing repetition of previous tokens in the context. Encourages new ideas to get generated. Unlike frequency penalty, this is a one-off application. tldr; repetition penalty, but additive.</p> </li> </ul> <p>Penalty Range -</p> <p>Note</p> <p>Unlike other backends, <code>0</code> disables penalties entirely!</p> <ul> <li> <p>API Request: <code>penalty_range</code> or <code>repetition_range</code> or <code>repetition_penalty_range</code></p> </li> <li> <p>Default: <code>-1</code></p> </li> <li> <p>When frequency OR presence penalty is enabled, a penalty_range value of <code>-1</code> applies the penalty to only the output tokens. A lower range is advised.</p> </li> <li> <p>Otherwise a penalty range value of <code>-1</code> = max sequence length</p> </li> <li> <p>Description: Amount of tokens to look behind when applying penalties.</p> </li> <li> <p>For frequency and presence penalty, this should be a low value to avoid \"backing the model into a corner\" when selecting similar tokens, resulting in large amounts of synonym repeats (aka \"thesaurus mode\").</p> </li> </ul>"},{"location":"Sampling/#alphabet-soup","title":"Alphabet Soup","text":"<p>Top-P -</p> <ul> <li> <p>API request field: <code>top_p</code></p> </li> <li> <p>Default: <code>1.0</code> - Off</p> </li> </ul> <p>Min-P -</p> <ul> <li> <p>API request field: <code>min_p</code></p> </li> <li> <p>Default: <code>0.0</code> - Off</p> </li> </ul> <p>Top-K -</p> <ul> <li> <p>API request field: <code>top_k</code></p> </li> <li> <p>Default: <code>0.0</code> - Off</p> </li> </ul> <p>Top-A -</p> <ul> <li> <p>API request field: <code>top_a</code></p> </li> <li> <p>Default: <code>0.0</code> - Off</p> </li> </ul>"},{"location":"Sampling/#miscellaneous","title":"Miscellaneous","text":"<p>Temperature -</p> <ul> <li> <p>API request field: <code>temperature</code></p> </li> <li> <p>Default: <code>1.0</code> - Off</p> </li> <li> <p>Description: A constant value applied to softmax calculation. A higher temperature = more randomness when choosing the next token.</p> </li> </ul> <p>Temp last -</p> <ul> <li> <p>API request field: <code>temp_last</code></p> </li> <li> <p>Default: <code>false</code> - Off</p> </li> <li> <p>Description: Places temperature application last in the sampling stack. Necessary for min-P sampling.</p> </li> </ul> <p>Typical -</p> <ul> <li> <p>API request field: <code>typical</code></p> </li> <li> <p>Default: <code>1.0</code> - Off</p> </li> </ul> <p>Tail-free Sampling -</p> <ul> <li> <p>API request field: <code>tfs</code></p> </li> <li> <p>Default: <code>1.0</code> - Off</p> </li> </ul> <p>Logit bias -</p> <ul> <li> <p>API request field: <code>logit_bias</code></p> </li> <li> <p>Default: <code>None</code> - Off</p> </li> <li> <p>Example: <code>[{\"1\": 50}, {\"2\": 75}]</code> - An array of bias objects</p> </li> <li> <p>Description: Adds a positive or negative value to change the occurrence of a specific token. Format: <code>{\"token\": bias}</code> where bias is from <code>-100</code> to <code>100</code>.</p> </li> </ul> <p>Mirostat mode -</p> <ul> <li> <p>API request field: <code>mirostat_mode</code></p> </li> <li> <p>Default: <code>0</code> - Off</p> </li> <li> <p>Exllamav2 only applies mirostat when <code>mirostat mode = 2</code></p> </li> </ul> <p>Mirostat tau -</p> <ul> <li> <p>API request field: <code>mirostat_tau</code></p> </li> <li> <p>Default: <code>1.5</code> - Off unless mirostat_mode = 2</p> </li> </ul> <p>Mirostat eta -</p> <ul> <li> <p>API request field: <code>mirostat_eta</code></p> </li> <li> <p>Default: <code>0.1</code> - Off unless mirostat_mode = 2</p> </li> </ul>"},{"location":"Server-options/","title":"Configuration","text":"<p>TabbyAPI primarily uses a config.yml file to adjust various options. This is the preferred way and has the ability to adjust all options of TabbyAPI.</p> <p>CLI arguments are also included, but those serve to override the options set in config.yml. Therefore, they act a bit differently compared to other programs, especially with booleans. Example: A user sets <code>gpu_split_auto</code> to True. The CLI arg will then be <code>--gpu_split-auto False</code> to override that previous config.yml setting.</p> <p>In addition, some config.yml options are too complex to represent as command args, so those are not included with the argparser.</p> <p>All of these options have descriptive comments above them. You should not need to reference this documentation page unless absolutely necessary.</p>"},{"location":"Server-options/#networking-options","title":"Networking Options","text":"Config Option Type (Default) Description host String (127.0.0.1) Set the IP address used for hosting TabbyAPI port Int (5000) Set the TCP Port use for TabbyAPI disable_auth Bool (False) Disables API authentication send_tracebacks Bool (False) Send server tracebacks to client.Note: It's not recommended to enable this if sharing the instance with others. api_servers List[String] ([\"OAI\"]) API servers to enable. Possible values <code>\"OAI\", \"Kobold\"</code>"},{"location":"Server-options/#logging-options","title":"Logging Options","text":"<p>Note: With CLI args, all logging parameters are prefixed by <code>log-</code>. For example, <code>prompt</code> will be <code>--log-prompt true/false</code>.</p> Config Option Type (Default) Description prompt Bool (False) Logs prompts to the console generation_params Bool (False) Logs request generation options to the console requests Bool (False) Logs a request's URL, Body, and Headers to the console"},{"location":"Server-options/#sampling-options","title":"Sampling Options","text":"<p>Note: This block is for sampling overrides, not samplers themselves.</p> Config Option Type (Default) Description override_preset String (None) Startup the given sampler override preset in the sampler_overrides folder"},{"location":"Server-options/#developer-options","title":"Developer Options","text":"<p>Note: These are experimental flags that may be removed at any point.</p> Config Option Type (Default) Description unsafe_launch Bool (False) Skips dependency checks on startup. Only recommended for debugging. disable_request_streaming Bool (False) Forcefully disables streaming requests cuda_malloc_backend Bool (False) Uses pytorch's CUDA malloc backend to load models. Helps save VRAM.Safe to enable. uvloop Bool (False) Use a faster asyncio event loop. Can increase performance.Safe to enable. realtime_process_priority Bool (False) Set the process priority to \"Realtime\". Administrator/sudo access required, otherwise the priority is set to the highest it can go in userland."},{"location":"Server-options/#model-options","title":"Model Options","text":"<p>Note: Most of the options here will only apply on initial model load/startup (ephemeral). They will not persist unless you add the option name to <code>use_as_default</code>.</p> Config Option Type (Default) Description model_dir String (\"models\") Directory to look for models.Note: Persisted across subsequent load requests use_dummy_models Bool (False) Send a dummy OAI model card when calling the <code>/v1/models</code> endpoint. Used for clients which enforce specific OAI models.Note: Persisted across subsequent load requests model_name String (None) Folder name of a model to load. The below parameters will not apply unless this is filled out. use_as_default List[String] ([]) Keys to use by default when loading models. For example, putting <code>cache_mode</code> in this array will make every model load with that value unless specified by the API request.Note: Also applies to the <code>draft</code> sub-block max_seq_len Float (None) Maximum sequence length of the model. Uses the value from config.json if not specified here. override_base_seq_len Float (None) Overrides the base sequence length of a model. You probably don't want to use this. max_seq_len is better.Note: This is only required for automatic RoPE alpha calculation AND if the model has an incorrect base sequence length (ex. Mistral 7b) tensor_parallel Bool (False) Use tensor parallelism to load the model. This ignores the value of gpu_split_auto. gpu_split_auto Bool (True) Automatically split the model across multiple GPUs. Manual GPU split isn't used if this is enabled. autosplit_reserve List[Int] ([96]) Amount of empty VRAM to reserve when loading with autosplit.Represented as an array of MB per GPU used. gpu_split List[Float] ([]) Float array of GBs to split a model between GPUs. rope_scale Float (1.0) Adjustment for rope scale (or compress_pos_emb) rope_alpha Float (None) Adjustment for rope alpha. Leave blank to automatically calculate based on the max_seq_len. cache_mode String (\"FP16\") Cache mode for the model.Options: FP16, Q8, Q6, Q4 cache_size Int (max_seq_len) Note: If using CFG, the cache size should be 2 * max_seq_len. chunk_size Int (2048) Amount of tokens per chunk with ingestion. A lower value reduces VRAM usage at the cost of ingestion speed. max_batch_size Int (None) The absolute maximum amount of prompts to process at one time. This value is automatically adjusted based on cache size. prompt_template String (None) Name of a jinja2 chat template to apply for this model. Must be located in the <code>templates</code> directory. num_experts_per_token Int (None) Number of experts to use per-token for MoE models. Pulled from the config.json if not specified. fasttensors Bool (False) Possibly increases model loading speeds."},{"location":"Server-options/#draft-model-options","title":"Draft Model Options","text":"Config Option Type (Default) Description draft_model_dir String (\"models\") Directory to look for draft models.Note: Persisted across subsequent load requests draft_model_name String (None) String: Folder name of a draft model to load. draft_rope_scale Float (1.0) String: RoPE scale value for the draft model. draft_rope_alpha Float (1.0) RoPE alpha value for the draft model. Leave blank for auto-calculation. draft_cache_mode String (\"FP16\") Cache mode for the draft model.Options: FP16, Q8, Q6, Q4"},{"location":"Server-options/#lora-options","title":"Lora Options","text":"Config Option Type (Default) Description lora_dir String (\"loras\") Directory to look for loras.Note: Persisted across subsequent load requests loras List[loras] ([]) List of lora objects to apply to the model. Each object contains a name and scaling. name String (None) Folder name of a lora to load.Note: An element of the <code>loras</code> key scaling Float (1.0) \"Weight\" to apply the lora on the parent model. For example, applying a lora with 0.9 scaling will lower the amount of application on the parent model.Note: An element of the <code>loras</code> key"},{"location":"Server-options/#embeddings-options","title":"Embeddings Options","text":"<p>Note: Most of the options here will only apply on initial embedding model load/startup (ephemeral).</p> Config Option Type (Default) Description embedding_model_dir String (\"models\") Directory to look for embedding models.Note: Persisted across subsequent load requests embeddings_device String (\"cpu\") Device to load an embedding model on.Options: cpu, cuda, autoNote: Persisted across subsequent load requests embedding_model_name String (None) Folder name of an embedding model to load using infinity-emb."},{"location":"Sharing/","title":"Remote Access","text":"<p>This page of the documentation serves to illustrate the in-between case of exposing a local instance without having to pay for a domain and set up a complicated reverse proxy.</p>"},{"location":"Sharing/#ngrok","title":"Ngrok","text":"<p>Ngrok is the recommended method for sharing a local instance over the internet. Throughput is faster than the alternatives, but there are limits on their free tier.</p> <p>To get started: 1. Sign up and install ngrok using instructions from their website    1. The ngrok for Windows install uses chocolatey, a package manager that isn't installed on Windows by default. Instead, use the winget package manager to install ngrok instead by running <code>winget install --id=Ngrok.Ngrok -e</code> 2. Start your TabbyAPI instance 3. In a separate terminal, open an ngrok instance using the command <code>ngrok http 5000</code> (or the port that TabbyAPI is running on) 4. Copy the public URL that shows up and send that to your users.</p>"},{"location":"Sharing/#cloudflared","title":"Cloudflared","text":"<p>Cloudflared is a free local tunneling service provided by Cloudflare. Free URLs do not have restrictions, but clients will have slower throughput.</p> <p>To get started: 1. Download the cloudflared binary from their releases 2. Open a terminal to where you downloaded cloudflared. 3. Start your TabbyAPI instance 4. Run <code>./cloudflared tunnel --url 127.0.0.1:5000</code> to start a quick tunnel on that port 5. Copy the provided HTTPS url and send that to your users.</p>"},{"location":"Sharing/#tailscale","title":"Tailscale","text":"<p>Tailscale is a product that uses the WireGuard protocol to provide a mesh network VPN for connecting your devices anywhere you go. Think of it as a private LAN that's accessible from anywhere.</p> <p>Note</p> <p>This is not a method for exposing your TabbyAPI instance to the world. If you want that, use the other two services or look into tailscale funnels.</p> <p>To get started: 1. Set your TabbyAPI IP to <code>0.0.0.0</code> otherwise you will not be able to access your instance outside your local machine. 2. Sign up and get started on Tailscale's website, then install the client. 3. Connect to your tailscale account on both your host and client machine. 4. Select the Tailscale icon (usually in the system tray) and get the name of your host device. This is usually the same as the hostname. 5. You can now access your TabbyAPI instance via <code>hostname:5000</code> instead of localhost as long as your are connected to your tailnet.</p>"},{"location":"Usage/","title":"Usage","text":"<p>TabbyAPI's main use-case is to be an API server for running ExllamaV2 models.</p>"},{"location":"Usage/#api-server","title":"API Server","text":"<p>Currently TabbyAPI supports clients that use the OpenAI standard and KoboldAI's API.</p> <p>In addition, there are expanded parameters to generation endpoints along with administrative endpoints for loading, unloading, loras, sampling overrides, etc.</p> <p>Note</p> <p>If you are a developer and want to add full TabbyAPI support to your app, it's recommended to use the autogenerated documentation.</p> <p>Below is an example CURL request using the OpenAI completions endpoint:</p> <pre><code>curl http://localhost:5000/v1/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"model\": \"meta-llama/Meta-Llama-3-8B\",\n  \"prompt\": \"Once upon a time,\",\n  \"max_tokens\": 400,\n  \"stream\": false,\n  \"min_p\": 0.05,\n  \"repetition_penalty\": 1.05\n}'\n</code></pre>"},{"location":"Usage/#authentication","title":"Authentication","text":"<p>Every call to a TabbyAPI endpoint requires some form of authentication. Keys have two types of permissions: - API: Accesses non-invasive endpoints (ex. generation, model list fetching) - Admin: Allowed to access protected endpoints that deal with resources (ex. loading, unloading)</p> <p>In addition, when calling list endpoints, API keys will only fetch the currently loaded object while admin keys will list the entire directory. For example, calling <code>/v1/models</code> will return a list of the user-configured models directory only if an admin key is passed.</p> <p>Therefore, it's recommended to keep the admin key for yourself and only share the api key with users.</p> <p>If these keys get compromised, shut down your server, delete the <code>api_tokens.yml</code> file, and restart. This will generate new keys which you can share with users.</p> <p>To bypass authentication checks, set <code>disable_auth</code> to <code>True</code> in config.yml. However, turning off authentication without a third-party solution will make your instance open to the world.</p>"},{"location":"Usage/#difficult-to-get-started","title":"Difficult to get started?","text":"<p>Is the API difficult? Don't want to load models with <code>config.yml</code>? That's okay! Not everyone is a master user of AI products when starting out. </p> <p>For newer users, it's recommended to use a UI that allows for managing TabbyAPI via API endpoints.</p> <p>To find UI projects, take a look at Community Projects for more information.</p> <p>The Discord is also a great place to ask for help. Please be nice when asking questions as all the developers are volunteers who have lives outside of TabbyAPI. </p>"}]}